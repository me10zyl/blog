---
title: Spark 快速入门教程（一） - Spark 体验
tags: spark
date: 2017-05-08 15:37:03
---

Spark 入门教程, 下载与安装，Spark Shell 的使用。

<!-- more -->

# Spark 是什么

Spark 是一个用来实现快速而通用的**集群计算平台**。

相比 MapReduce 计算模型，Spark 的一个主要特点就是能够在内存中进行计算，因而更快，不过即使是必须在磁盘上进行的复杂方面，在速度方面比 MapReduce 更加高效。

Spark 所提供的接口非常丰富。除了提供基于 Python、 Java、 Scala、 R 和 SQL 的简单易用的API以及内建的丰富程序以外， Spark 还能和其他大数据工具密切配合使用。 例如，Spark 可以运行在 Hadoop 集群上，访问包括 Cassandra 在内的任意 Hadoop 数据源。

Spark 支持为各种不同的应用场景专门设计的高级组件，比如SQL和机器学习等。
![](/images/spark/spark-01.png)

# Spark 下载与安装

Spark 现属于 apache 开源软件基金会，访问 [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html) 下载。


``` bash
wget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz
tar -xzf spark-2.1.1-bin-hadoop2.7.tgz
```

# 运行 Spark 例子和 Shell

切换到 Spark 目录下，运行例子显示 Pi 的10位小数。

``` bash
./bin/run-example SparkPi 10
```

运行 Spark Shell

``` bash
./bin/spark-shell
```

``` bash
scala> val lines = sc.textFile("README.md") //读取 Spark 目录下的 README.md
scala> lines.count() //返回文档总行数
scala> lines.first() //返回文档第一行
```

退出 Shell `Ctrl + D`

变量 lines 是一个RDD，是从你的电脑上的本地文件读入的文本文件创建出来的。


参考资料
---
- 《Spark 快速大数据分析》
- [Spark 官方文档](http://spark.apache.org/docs/latest/)


下一篇，将介绍如何使用独立Java程序来操作RDD。
